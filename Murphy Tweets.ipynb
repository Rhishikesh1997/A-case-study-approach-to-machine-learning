{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Fetch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import time\n",
    "import csv\n",
    "#from tweepy import twitter_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key=\"ruNYgRO4DnQZzZnxEWy6KoOCB\"\n",
    "consumer_secret=\"uc3kJgjj51bYXvEBobUoxzuhKc5InioIJPCQ8jEn6mmisJEtcO\"\n",
    "access_key=\"4673892211-uXEjUOfqZoWSqfYEWhSQpclwdURzZRTO2LaAd6N\"\n",
    "access_secret=\"Wq2uTSMNiJ9JJ40dbmPsnBZsKpDD1eFCRGV9Xp0eNZCaN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract tweets:\n",
    "def get_tweets(username):\n",
    "    # Authorization to consumer key and consumer secret\n",
    "    auth=tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "    #Access to user's access key and access secret\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    #calling api\n",
    "    api=tweepy.API(auth)\n",
    "    #3200 tweets to be extracted\n",
    "    number_of_tweets=3200\n",
    "    tweets=api.user_timeline(screen_name=username)\n",
    "    #Empty array\n",
    "    tmp=[]\n",
    "    ##Create array of tweet information: username,tweet id, date/time, text\n",
    "    tweets_for_csv=[tweet.text for tweet in tweets]   #csv file created\n",
    "    for j in tweets_for_csv:\n",
    "        #Appending tweets to the empty array tmp\n",
    "        tmp.append(j)\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The ground is drying out nicely at Roscommon so it's unlikely to suit any soft ground specialists. A few each-way s‚Ä¶ https://t.co/aC7emJcsdy\", 'Gorgeous evening in The Rower, Co Kilkenny. Little poser üòç. https://t.co/H5F6dz4M8x', \"@irishracing @WexfordRacecour @LizDoyleRacing Both horses and jockeys were okay. We wouldn't publish it otherwise.\", 'That break again üòçüòç.\\nHope Pestana goes all the way.\\n#BoyleSportsIGD https://t.co/5Gmc6XCcro', 'The sixth in Wexford this afternoon. Poetry in motion, bar a few out the back üòÅ https://t.co/srpLZro7o4', 'Another afternoon at the beautiful Curragh. Check out my formscans by following the link \\nhttps://t.co/eWEDSEoZkf https://t.co/1mbHI0G2wM', \"@SineadCashin @RTERadio1 @RTEdoconone Never heard of that story. Thanks Sinead, I'll tune in.\", \"@Gavbrophy I do indeed. Bellewstown is probably the closest you'd get to being at a race meeting nowadays.\", '@Gavbrophy You could go to Bellewstown tomorrow. There is ample space for viewing the races just beside the enclosures.', 'Last race abandoned at Cork. Parts of the track unfit for racing.', 'Of course it started to lash when I went out to walk the sprint track. ‚òî https://t.co/1xFkRc6egJ', \"I'm surprised Cork got the go ahead today. Dangerous driving conditions on route here, with some trees down. Very windy on-course now.\", 'Not a great day weather wise at the Curragh. Jessica Harrington looks to have a few with strong claims. All eyes on‚Ä¶ https://t.co/h2tHklPOp7', 'Saw this in Mayo yesterday. Only in Ireland üòÜ \\n#Covid_19 #coronavirus https://t.co/Nv0mbqL7MG', 'The bend going away from the stand looks much better today at Sligo. Stat of the day: Paul Townend has won on 7 of‚Ä¶ https://t.co/0D09yOZuZQ', 'Passing through Mountbellew, Co Galway and I had to get a pic of the lovely Bobbyjo statue. Irish National winner i‚Ä¶ https://t.co/bPI6AFgfUI', 'They are not racing on this part of the track at Roscommon but it just shows how much rain fell here. The outer tra‚Ä¶ https://t.co/D9xunceqLf', 'Thanks to @TramoreRaces for looking after us well over the last four days. The hot food available was very tasty to‚Ä¶ https://t.co/zFa03P3Qb4', \"@RichardMForde I can't be giving preferential treatment now, that wouldn't be fair üòÅ\", 'Merrior is actually another solid each-way prospect in the opener. He flew home last time at Galway.']\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    get_tweets(\"@1DonalMurphy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some basic libararies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "tw=pd.read_excel(r'C:\\Users\\hp\\Downloads\\twitter.xlsx')\n",
    "tw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                              \n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fetching important variable\n",
    "tw=DataFrame(tw,columns=['user_id','created_at','screen_name','text','display_text_width'])  \n",
    "tw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean the tweets\n",
    "def cleanTxt(text):\n",
    "    text=re.sub(r'@[A-Za-z0-9]+','',text)   ##Removed @mentions\n",
    "    text=re.sub(r'#','',text)   ##Remove hashtags\n",
    "    text=re.sub(r'RT[\\s]+','',text)   ##Removing RT\n",
    "    text=re.sub(r'https?\\/\\/\\S+','',text)  ##Remove hyperlink\n",
    "    text=re.sub(r\"\\'\", \"\", text) \n",
    "    text=re.sub(r'\\s+', ' ', text) \n",
    "    text=re.sub(r'\\S*@\\S*\\s?', '', text) \n",
    "    return text\n",
    "\n",
    "##Cleaning the text\n",
    "tw['text']=tw['text'].apply(cleanTxt)\n",
    "tw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set some english stopwords\n",
    "from wordcloud import STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"http\")\n",
    "stopwords.add(\"https\")\n",
    "stopwords.add(\"amp\")\n",
    "stopwords.add(\"im\")\n",
    "stopwords.add(\"co\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer    #from sklearn module imported countvectorizer function\n",
    "new_vect=CountVectorizer(ngram_range=(1,2),stop_words=stopwords)   ###Fixed the Vectorizer\n",
    "new_vect    ##print vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation      \n",
    "doc_term_matrix=new_vect.fit_transform(tw['text'].values.astype('U'))    ###made the document-term-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying LDA and fitting document term matrix in it.\n",
    "LDA=LatentDirichletAllocation(n_components=10,random_state=0)  \n",
    "LDA.fit(doc_term_matrix)                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculated Ist and top topic words document term matrix in array format.\n",
    "first_topic=LDA.components_[0]\n",
    "print(\"first_topic:\",first_topic)\n",
    "top_topic_words=first_topic.argsort()[-10:]\n",
    "print(\"top_topic_words:\",top_topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Showing Top Topic Wrods from vectorizer.\n",
    "for i in top_topic_words:\n",
    "    print(new_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Showing Top 10 Topics from tweets and in that 10 topics 10 top words. \n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([new_vect.get_feature_names()[i] \n",
    "           for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning Topics to every texts i.e which topic number comes under which twitts.\n",
    "topic_values=LDA.transform(doc_term_matrix)\n",
    "topic_values.shape\n",
    "tw['topic']=topic_values.argmax(axis=1)\n",
    "tw=DataFrame(tw,columns=['user_id','created_at','screen_name','text','display_text_width','topic'])  ## Adding topic Variable into DataFrame\n",
    "tw.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
